---
title: "PI2_V2"
subtitle: "Clustering"
author: Groupe 88
date: "`r format(Sys.time())`"
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    theme: flatly
    highlight: espresso
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Dataset

```{r}
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering algorithms & visualization
library(FactoMineR)
library(gridExtra)
library(fpc)
library(dbscan)
library(mclust)
```

## Importation

```{r}
df = read.csv("Last.csv", row.names = 1, sep = ';')
```

Omit non available values if there are any left (pre-treated dataset)
```{r}
df <- na.omit(df)
```

Feature scaling through standardization is an important preprocessing step for many machine learning algorithms. Standardization involves rescaling the features such that they have the properties of a standard normal distribution with a mean of 0 and a standard deviation of 1. It helps to normalise the data within a particular range and it can also help in speeding up the calculations in the algorithms.
```{r}
df <- scale(df)
head(df)
```

#Outlier Treatment

Outliers can drastically bias/change the fit estimates and predictions. 

##Detect outliers

Multivariate Model Approach

Declaring an observation as an outlier based on just one feature could lead to unrealistic inferences. When you have to decide if an individual entity is an extreme value or not, it is better to collectively consider the features (X's) that matter -> Using Cook's Distance.

Cook's distance is a measure computed with respect to a given regression model and therefore is impacted only by the X variables included in the model. But, what does cook's distance mean? It computes the influence exerted by each data point (row) on the predicted outcome.

The cook's distance for each observation i measures the change in Y^ (fitted Y) for all observations with and without the presence of observation i, so we know how much the observation i impacted the fitted values. Mathematically, cook's distance Di for observation i is computed as:

Di=???nj=1(Y^j???Y^j(i))^2/p*MSE

where : 

Y^j is the value of jth fitted response when all the observations are included.
Y^j(i) is the value of jth fitted response, where the fit does not include observation i.
MSE is the mean squared error.
p is the number of coefficients in the regression model.

```{r}
new.df = as.data.frame.matrix(df)

mod <- lm( new.df$Pre.Tax.ROE ~ ., data = new.df)
cooksd <- cooks.distance(mod)
```

In general use, the observations that have a cook's distance greater than 4 times the mean may be classified as influential.
```{r}
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance
abline(h = 4*mean(cooksd, na.rm=T), col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4*mean(cooksd, na.rm=T),names(cooksd),""), col="red")  # add labels

influential <- names(cooksd)[(cooksd > 4*mean(cooksd, na.rm=T))]
new.df[influential, ]
```

##Outliers Test

The function outlierTest from car package gives the most extreme observation based on the given model.
```{r}
car::outlierTest(mod)
```

##Clustering Based Outlier Detection Technique

The function takes for parameters a dataframe, the number of outliers to detect and the number of centers for the k-means clustering.

First, it will do a PCA :
-statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.
-consiste à transformer des variables corrélées en nouvelles variables décorrélées les unes des autres. Ces nouvelles variables sont nommées « composantes principales », ou axes principaux. Elle permet de réduire le nombre de variables et de rendre l'information moins redondante.

In the next step we will calculate the distance (Euclidean) between the objects and cluster centers to determine the outliers and identify n largest distances which are outliers. 

Finally we plot the graph with c clusters et n outliers detected we can remove later on for our final clustering.
```{r}
Outliers = function(Df, n, c)
{
  #parameters : Df = dataframe, n = number of outliers, c = number of centers for clustering
  PCA_Data = princomp(Df, cor = T)
  PC1 = PCA_Data$scores[,1]
  PC2 = PCA_Data$scores[,2]

  X = cbind(PC1, PC2)
  km = kmeans(X, centers = c)
  
  centers = km$centers[km$cluster,]
  distances = sqrt(rowSums((X-centers)^2))
  outliers = order(distances, decreasing = T)[1:n]

  plot(X, pch = 16, col = km$cluster, cex = 1, main = paste0("C = ", c, "     N = ", n))
  points(km$centers, pch = 23, bg = "yellow", cex = 2, lwd = 2)
  points(X[outliers,], pch = 25, col = "orange", cex = 2)
  legend("topleft", legend = c("Cluster center", "Outliers"), pt.cex = 2, pch = c(23, 25), col = c("black", "orange"), pt.bg = c("yellow", NA))
  
  return (outliers)
}

test = Outliers(df,20,7)
```

```{r}
distance <- get_dist(df)
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))

k2 <- kmeans(df, centers = 2, nstart = 25)
str(k2)
k2

p1 = fviz_cluster(k2, data = df) + ggtitle("k = 2")

k3 = kmeans(df, centers = 3, nstart = 25)
p2 = fviz_cluster(k3, data = df) + ggtitle("k = 3")

k4 = kmeans(df, centers = 4, nstart = 25)
p3 = fviz_cluster(k4, data = df) + ggtitle("k = 4")

grid.arrange(p1, p2, p3, nrow = 2)


set.seed(123)

fviz_nbclust(df, kmeans, method = "wss", k.max = 20)

fviz_nbclust(df, kmeans, method = "silhouette", k.max = 20)

fviz_nbclust(df, kmeans, method = "gap_stat", k.max = 20)

# Compute PAM
pam <- pam(df, 4) 
# Visualize
a2 = fviz_cluster(pam) + ggtitle("PAM")

# Compute CLARA
clara <- clara(df, 4, samples = 50, pamLike = TRUE)
# Visualize
a3 = fviz_cluster(clara) + ggtitle("CLARA")

grid.arrange(p3, a2, a3, nrow = 2)


PCA_Data = princomp(df, cor = T)
str(PCA_Data)
summary(PCA_Data)

PC1 <- PCA_Data$scores[,1]
PC2 <- PCA_Data$scores[,2]

X <- cbind(PC1, PC2)
km <- kmeans(X, centers = 4)
plot(PC1, PC2, col = km$cluster, xlab = "PC1", ylab = "PC2", main = "K-means clustering with PCA")
points(km$centers, col = 1:3, pch = 3, cex = 2, lwd = 3)
```













